{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948578f6-bf6c-421a-b7df-9fceb8ce6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import fire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116f6210-e97f-4bf8-8dfb-fd6b1648f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770a0a27-a962-4e0d-879d-e6ae48285ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffb6d28-aad0-45df-a9aa-42ad0f336e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, g, b, eps: float = 1e-5):\n",
    "    # print(\"Layer Norm Input Shapes:\")\n",
    "    # print(f\"x: {x}\")\n",
    "    # print(f\"x shape: {x.shape}\")\n",
    "    # print(f\"g shape: {g.shape}\")\n",
    "    # print(f\"b shape: {b.shape}\")\n",
    "    \n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    #print(f\"mean: {mean}\")\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "   # print(f\"variance: {variance}\")\n",
    "    \n",
    "    x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "  #  print(f\"x_normalized: {x_normalized.shape}\")\n",
    "    \n",
    "    result = g * x_normalized + b\n",
    "   # print(f\"result: {result}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741537c-3c59-4c6b-bdc3-9bd1294b6766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c1a8cb-1e70-43c0-bc92-d188fdcb5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, w, b):\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc5bd18-6d42-4235-a8ed-9f95c113d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(x, c_fc, c_proj):\n",
    "    a = gelu(linear(x, **c_fc))\n",
    "    x = linear(a, **c_proj)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30d9ce0-cd7b-42e9-adc2-d3e470948d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention(q, k, v, mask):\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eee783d-4159-4e12-85b1-da2127c6e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mha(x, c_attn, c_proj, n_head=1):\n",
    "    x = linear(x, **c_attn)\n",
    "    qkv = np.split(x, 3, axis=-1)\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
    "    x = np.hstack(out_heads)\n",
    "    x = linear(x, **c_proj)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e59a61-bbe0-4954-a06f-ed1f1037860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
    "    x =  x +  mha(x, **attn, n_head=n_head)\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812d6c8-7fa4-40cb-97c4-5b68f7a0aab4",
   "metadata": {},
   "source": [
    "## Custom code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d3a81a-59bd-44f3-a40a-0639dac58c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 10:57:10.707758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-04 10:57:11.594018: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-04 10:57:11.594062: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-04 10:57:14.157569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-04 10:57:14.157994: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-04 10:57:14.158012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_encoder_hparams_and_params\n",
    "encoder, hparams, params = load_encoder_hparams_and_params(\"124M\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c879cdd-a2da-464c-b5e2-06cbd974d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[439, 10281, 5806, 1451, 274]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = encoder.encode(\"all heroes wear capes\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1df13d85-2c01-4585-8084-ed40428d3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks[:1]:\n",
    "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # projection to vocab\n",
    "    #x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x #@ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420f3dff-c28c-4a08-8fb0-b01fb91a4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = gpt2(inputs, **params, n_head=1)  # model forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd83e0c-73bb-4be3-b7cf-65533f66e5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6a4c28-739c-40ca-836a-b9093379f1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24636951,  0.83114105,  4.452487  , ..., -0.08223414,\n",
       "         0.1122146 , -0.95171577],\n",
       "       [-0.93640256,  0.46588486,  4.2565317 , ...,  0.4348594 ,\n",
       "        -0.54787946, -0.78516173],\n",
       "       [-0.6980567 ,  0.6011211 ,  4.7558928 , ...,  0.06484202,\n",
       "        -0.28358984, -0.60146976],\n",
       "       [-0.82171917,  0.8055482 ,  4.5270495 , ...,  0.14451209,\n",
       "        -0.41624647, -0.5769494 ],\n",
       "       [-1.4169543 ,  0.9740439 ,  4.69918   , ...,  0.2777383 ,\n",
       "        -0.60946417, -0.36688083]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1e5e35-012a-4339-8579-15a45517ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################end custom code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b84196f-24b9-4da5-80bd-9e5ee2180141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # projection to vocab\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45ca6755-ef51-42a3-bce3-851a91e64288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "        next_id = np.argmax(logits[-1])\n",
    "        inputs.append(int(next_id))\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50827cb3-1cd8-44dc-8636-90b024a2d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(prompt: str, n_tokens_to_generate: int = 40, model_size: str = \"124M\", models_dir: str = \"models\"):\n",
    "    from utils import load_encoder_hparams_and_params\n",
    "\n",
    "    # load encoder, hparams, and params from the released open-ai gpt-2 files\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
    "\n",
    "    # encode the input string using the BPE tokenizer\n",
    "    input_ids = encoder.encode(prompt)\n",
    "\n",
    "    # make sure we are not surpassing the max sequence length of our model\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "\n",
    "    # generate output ids\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "    # decode the ids back into a string\n",
    "    output_text = encoder.decode(output_ids)\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "babcfac3-8b10-4be5-98f7-46fd54b00e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|███████████████████████████████| 15/15 [00:03<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  Mock EG satire MT MTlev EG prediction prediction prediction prediction� Nordic Nordic Nordic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"all heroes wear capes\"\n",
    "generated_text = main(prompt, n_tokens_to_generate=15)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6393c-ff9d-4bc0-a66d-5ef60cfd991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```\n",
    "\n",
    "## Running from Command Line\n",
    "\n",
    "If you want to run this script from the command line, you can use the following cell:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n",
    "```\n",
    "\n",
    "Note: Make sure you have the `fire` library installed (`pip install fire`) to use the command-line interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
